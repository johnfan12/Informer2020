# ===============================================================================================
# The following shows the last analyze fail log message.
# ===============================================================================================

----------------------------------------------------
- Caught exception:
----------------------------------------------------
The local variable 'dec_inp' is not defined in false branch, but defined in true branch.
In file E:\Codearea\AIbase\Informer2020\exp\exp_informer_mindspore.py:270
    def _process_one_batch(self, dataset_object, batch_x, batch_y, batch_x_mark, batch_y_mark):
        batch_x = batch_x.astype(ms.float32)
        batch_y = batch_y.astype(ms.float32)
        batch_x_mark = batch_x_mark.astype(ms.float32)
        batch_y_mark = batch_y_mark.astype(ms.float32)

        # decoder input
        if self.args.padding==0:
            dec_inp = ops.zeros_like(batch_y[:, -self.args.pred_len:, :])
        elif self.args.padding==1:
            dec_inp = ops.ones_like(batch_y[:, -self.args.pred_len:, :])
                      ~<-------------HERE

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\ccsrc\pipeline\jit\ps\parse\function_block.cc:654 mindspore::parse::FunctionBlock::CheckVariableNotDefined

----------------------------------------------------
- The Traceback of Net Construct Code:
----------------------------------------------------
# 0 In file E:\Codearea\AIbase\Informer2020\.venv\Lib\site-packages\mindspore\ops\composite\base.py:597, 31~56
                        return grad_(fn, weights)(*args)
                               ^~~~~~~~~~~~~~~~~~~~~~~~~
# 1 In file E:\Codearea\AIbase\Informer2020\exp\exp_informer_mindspore.py:147, 25~48
            pred, true = self._process_one_batch(
                         ^~~~~~~~~~~~~~~~~~~~~~~

# ===============================================================================================
# The following shows the IR when the function graphs evaluation fails to help locate the problem.
# You can search the last ------------------------> to the node which is evaluated failure.
# Refer to https://www.mindspore.cn/search?inputValue=analyze_fail.ir to get more instructions.
# ===============================================================================================

# IR entry: @after_grad_6
# Total subgraphs: 0

# Total params: 69
# Params:
%para1_args0: <null>
%para2_args1: <null>
%para3_args2: <null>
%para4_args3: <null>
%para5_enc_embedding.value_embedding.tokenConv.weight: <Ref[Tensor[Float32]], (512, 7, 1, 3), ref_key=enc_embedding.value_embedding.tokenConv.weight>  :  has_default
%para6_enc_embedding.temporal_embedding.embed.weight: <Ref[Tensor[Float32]], (512, 4), ref_key=enc_embedding.temporal_embedding.embed.weight>  :  has_default
%para7_dec_embedding.value_embedding.tokenConv.weight: <Ref[Tensor[Float32]], (512, 7, 1, 3), ref_key=dec_embedding.value_embedding.tokenConv.weight>  :  has_default
%para8_dec_embedding.temporal_embedding.embed.weight: <Ref[Tensor[Float32]], (512, 4), ref_key=dec_embedding.temporal_embedding.embed.weight>  :  has_default
%para9_encoder.attn_layers.0.attention.query_projection.weight: <Ref[Tensor[Float32]], (512, 512), ref_key=encoder.attn_layers.0.attention.query_projection.weight>  :  has_default
%para10_encoder.attn_layers.0.attention.query_projection.bias: <Ref[Tensor[Float32]], (512), ref_key=encoder.attn_layers.0.attention.query_projection.bias>  :  has_default
%para11_encoder.attn_layers.0.attention.key_projection.weight: <Ref[Tensor[Float32]], (512, 512), ref_key=encoder.attn_layers.0.attention.key_projection.weight>  :  has_default
%para12_encoder.attn_layers.0.attention.key_projection.bias: <Ref[Tensor[Float32]], (512), ref_key=encoder.attn_layers.0.attention.key_projection.bias>  :  has_default
%para13_encoder.attn_layers.0.attention.value_projection.weight: <Ref[Tensor[Float32]], (512, 512), ref_key=encoder.attn_layers.0.attention.value_projection.weight>  :  has_default
%para14_encoder.attn_layers.0.attention.value_projection.bias: <Ref[Tensor[Float32]], (512), ref_key=encoder.attn_layers.0.attention.value_projection.bias>  :  has_default
%para15_encoder.attn_layers.0.attention.out_projection.weight: <Ref[Tensor[Float32]], (512, 512), ref_key=encoder.attn_layers.0.attention.out_projection.weight>  :  has_default
%para16_encoder.attn_layers.0.attention.out_projection.bias: <Ref[Tensor[Float32]], (512), ref_key=encoder.attn_layers.0.attention.out_projection.bias>  :  has_default
%para17_encoder.attn_layers.0.conv1.weight: <Ref[Tensor[Float32]], (2048, 512, 1, 1), ref_key=encoder.attn_layers.0.conv1.weight>  :  has_default
%para18_encoder.attn_layers.0.conv2.weight: <Ref[Tensor[Float32]], (512, 2048, 1, 1), ref_key=encoder.attn_layers.0.conv2.weight>  :  has_default
%para19_encoder.attn_layers.0.norm1.gamma: <Ref[Tensor[Float32]], (512), ref_key=encoder.attn_layers.0.norm1.gamma>  :  has_default
%para20_encoder.attn_layers.0.norm1.beta: <Ref[Tensor[Float32]], (512), ref_key=encoder.attn_layers.0.norm1.beta>  :  has_default
%para21_encoder.attn_layers.0.norm2.gamma: <Ref[Tensor[Float32]], (512), ref_key=encoder.attn_layers.0.norm2.gamma>  :  has_default
%para22_encoder.attn_layers.0.norm2.beta: <Ref[Tensor[Float32]], (512), ref_key=encoder.attn_layers.0.norm2.beta>  :  has_default
%para23_encoder.attn_layers.1.attention.query_projection.weight: <Ref[Tensor[Float32]], (512, 512), ref_key=encoder.attn_layers.1.attention.query_projection.weight>  :  has_default
%para24_encoder.attn_layers.1.attention.query_projection.bias: <Ref[Tensor[Float32]], (512), ref_key=encoder.attn_layers.1.attention.query_projection.bias>  :  has_default
%para25_encoder.attn_layers.1.attention.key_projection.weight: <Ref[Tensor[Float32]], (512, 512), ref_key=encoder.attn_layers.1.attention.key_projection.weight>  :  has_default
%para26_encoder.attn_layers.1.attention.key_projection.bias: <Ref[Tensor[Float32]], (512), ref_key=encoder.attn_layers.1.attention.key_projection.bias>  :  has_default
%para27_encoder.attn_layers.1.attention.value_projection.weight: <Ref[Tensor[Float32]], (512, 512), ref_key=encoder.attn_layers.1.attention.value_projection.weight>  :  has_default
%para28_encoder.attn_layers.1.attention.value_projection.bias: <Ref[Tensor[Float32]], (512), ref_key=encoder.attn_layers.1.attention.value_projection.bias>  :  has_default
%para29_encoder.attn_layers.1.attention.out_projection.weight: <Ref[Tensor[Float32]], (512, 512), ref_key=encoder.attn_layers.1.attention.out_projection.weight>  :  has_default
%para30_encoder.attn_layers.1.attention.out_projection.bias: <Ref[Tensor[Float32]], (512), ref_key=encoder.attn_layers.1.attention.out_projection.bias>  :  has_default
%para31_encoder.attn_layers.1.conv1.weight: <Ref[Tensor[Float32]], (2048, 512, 1, 1), ref_key=encoder.attn_layers.1.conv1.weight>  :  has_default
%para32_encoder.attn_layers.1.conv2.weight: <Ref[Tensor[Float32]], (512, 2048, 1, 1), ref_key=encoder.attn_layers.1.conv2.weight>  :  has_default
%para33_encoder.attn_layers.1.norm1.gamma: <Ref[Tensor[Float32]], (512), ref_key=encoder.attn_layers.1.norm1.gamma>  :  has_default
%para34_encoder.attn_layers.1.norm1.beta: <Ref[Tensor[Float32]], (512), ref_key=encoder.attn_layers.1.norm1.beta>  :  has_default
%para35_encoder.attn_layers.1.norm2.gamma: <Ref[Tensor[Float32]], (512), ref_key=encoder.attn_layers.1.norm2.gamma>  :  has_default
%para36_encoder.attn_layers.1.norm2.beta: <Ref[Tensor[Float32]], (512), ref_key=encoder.attn_layers.1.norm2.beta>  :  has_default
%para37_encoder.conv_layers.0.downConv.weight: <Ref[Tensor[Float32]], (512, 512, 1, 3), ref_key=encoder.conv_layers.0.downConv.weight>  :  has_default
%para38_encoder.conv_layers.0.norm.gamma: <Ref[Tensor[Float32]], (512), ref_key=encoder.conv_layers.0.norm.gamma>  :  has_default
%para39_encoder.conv_layers.0.norm.beta: <Ref[Tensor[Float32]], (512), ref_key=encoder.conv_layers.0.norm.beta>  :  has_default
%para40_encoder.norm.gamma: <Ref[Tensor[Float32]], (512), ref_key=encoder.norm.gamma>  :  has_default
%para41_encoder.norm.beta: <Ref[Tensor[Float32]], (512), ref_key=encoder.norm.beta>  :  has_default
%para42_decoder.layers.0.self_attention.query_projection.weight: <Ref[Tensor[Float32]], (512, 512), ref_key=decoder.layers.0.self_attention.query_projection.weight>  :  has_default
%para43_decoder.layers.0.self_attention.query_projection.bias: <Ref[Tensor[Float32]], (512), ref_key=decoder.layers.0.self_attention.query_projection.bias>  :  has_default
%para44_decoder.layers.0.self_attention.key_projection.weight: <Ref[Tensor[Float32]], (512, 512), ref_key=decoder.layers.0.self_attention.key_projection.weight>  :  has_default
%para45_decoder.layers.0.self_attention.key_projection.bias: <Ref[Tensor[Float32]], (512), ref_key=decoder.layers.0.self_attention.key_projection.bias>  :  has_default
%para46_decoder.layers.0.self_attention.value_projection.weight: <Ref[Tensor[Float32]], (512, 512), ref_key=decoder.layers.0.self_attention.value_projection.weight>  :  has_default
%para47_decoder.layers.0.self_attention.value_projection.bias: <Ref[Tensor[Float32]], (512), ref_key=decoder.layers.0.self_attention.value_projection.bias>  :  has_default
%para48_decoder.layers.0.self_attention.out_projection.weight: <Ref[Tensor[Float32]], (512, 512), ref_key=decoder.layers.0.self_attention.out_projection.weight>  :  has_default
%para49_decoder.layers.0.self_attention.out_projection.bias: <Ref[Tensor[Float32]], (512), ref_key=decoder.layers.0.self_attention.out_projection.bias>  :  has_default
%para50_decoder.layers.0.cross_attention.query_projection.weight: <Ref[Tensor[Float32]], (512, 512), ref_key=decoder.layers.0.cross_attention.query_projection.weight>  :  has_default
%para51_decoder.layers.0.cross_attention.query_projection.bias: <Ref[Tensor[Float32]], (512), ref_key=decoder.layers.0.cross_attention.query_projection.bias>  :  has_default
%para52_decoder.layers.0.cross_attention.key_projection.weight: <Ref[Tensor[Float32]], (512, 512), ref_key=decoder.layers.0.cross_attention.key_projection.weight>  :  has_default
%para53_decoder.layers.0.cross_attention.key_projection.bias: <Ref[Tensor[Float32]], (512), ref_key=decoder.layers.0.cross_attention.key_projection.bias>  :  has_default
%para54_decoder.layers.0.cross_attention.value_projection.weight: <Ref[Tensor[Float32]], (512, 512), ref_key=decoder.layers.0.cross_attention.value_projection.weight>  :  has_default
%para55_decoder.layers.0.cross_attention.value_projection.bias: <Ref[Tensor[Float32]], (512), ref_key=decoder.layers.0.cross_attention.value_projection.bias>  :  has_default
%para56_decoder.layers.0.cross_attention.out_projection.weight: <Ref[Tensor[Float32]], (512, 512), ref_key=decoder.layers.0.cross_attention.out_projection.weight>  :  has_default
%para57_decoder.layers.0.cross_attention.out_projection.bias: <Ref[Tensor[Float32]], (512), ref_key=decoder.layers.0.cross_attention.out_projection.bias>  :  has_default
%para58_decoder.layers.0.conv1.weight: <Ref[Tensor[Float32]], (2048, 512, 1, 1), ref_key=decoder.layers.0.conv1.weight>  :  has_default
%para59_decoder.layers.0.conv2.weight: <Ref[Tensor[Float32]], (512, 2048, 1, 1), ref_key=decoder.layers.0.conv2.weight>  :  has_default
%para60_decoder.layers.0.norm1.gamma: <Ref[Tensor[Float32]], (512), ref_key=decoder.layers.0.norm1.gamma>  :  has_default
%para61_decoder.layers.0.norm1.beta: <Ref[Tensor[Float32]], (512), ref_key=decoder.layers.0.norm1.beta>  :  has_default
%para62_decoder.layers.0.norm2.gamma: <Ref[Tensor[Float32]], (512), ref_key=decoder.layers.0.norm2.gamma>  :  has_default
%para63_decoder.layers.0.norm2.beta: <Ref[Tensor[Float32]], (512), ref_key=decoder.layers.0.norm2.beta>  :  has_default
%para64_decoder.layers.0.norm3.gamma: <Ref[Tensor[Float32]], (512), ref_key=decoder.layers.0.norm3.gamma>  :  has_default
%para65_decoder.layers.0.norm3.beta: <Ref[Tensor[Float32]], (512), ref_key=decoder.layers.0.norm3.beta>  :  has_default
%para66_decoder.norm.gamma: <Ref[Tensor[Float32]], (512), ref_key=decoder.norm.gamma>  :  has_default
%para67_decoder.norm.beta: <Ref[Tensor[Float32]], (512), ref_key=decoder.norm.beta>  :  has_default
%para68_projection.weight: <Ref[Tensor[Float32]], (7, 512), ref_key=projection.weight>  :  has_default
%para69_projection.bias: <Ref[Tensor[Float32]], (7), ref_key=projection.bias>  :  has_default

subgraph attr:
subgraph instance: after_grad_6 : 00000183723224B0
# In file E:\Codearea\AIbase\Informer2020\.venv\Lib\site-packages\mindspore\ops\composite\base.py:595~597, 20~56/                    @jit/
subgraph @after_grad_6() {
  %0(CNode_15) = resolve(NameSpace[Entry: 'after_grad'], after_grad)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
  %1(CNode_16) = MakeTuple(%para1_args0, %para2_args1, %para3_args2, %para4_args3)
      : (<Tensor[Float64], (32, 96, 7)>, <Tensor[Float64], (32, 72, 7)>, <Tensor[Float64], (32, 96, 4)>, <Tensor[Float64], (32, 72, 4)>) -> (<Tuple[Tensor[Float64]*4], TupleShape((32, 96, 7), (32, 72, 7), (32, 96, 4), (32, 72, 4))>)
      #scope: (Default)

#------------------------> 0
  %2(CNode_17) = DoUnpackCall(%0, %1)
      : (<Func, NoShape>, <Tuple[Tensor[Float64]*4], TupleShape((32, 96, 7), (32, 72, 7), (32, 96, 4), (32, 72, 4))>) -> (<null>)
      #scope: (Default)
  Return(%2)
      : (<null>)
      #scope: (Default)
      # In file E:\Codearea\AIbase\Informer2020\.venv\Lib\site-packages\mindspore\ops\composite\base.py:597, 24~56/                        return grad_(fn, weights)(*args)/
}
# Order:
#   1: @after_grad_6:CNode_15{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> Entry: 'after_grad', [2]: ValueNode<Symbol> after_grad}
#   2: @after_grad_6:CNode_17{[0]: ValueNode<Primitive> DoUnpackCall, [1]: CNode_15, [2]: CNode_16}
#   3: @after_grad_6:CNode_18{[0]: ValueNode<Primitive> Return, [1]: CNode_17}


subgraph attr:
core: 1
subgraph instance: UnpackCall_7 : 0000018372324080

subgraph @UnpackCall_7(%para0_Parameter_8, %para0_Parameter_9) {
  %0(CNode_19) = TupleGetItem(%para0_Parameter_9, I64(0))
      : (<Tuple[Tensor[Float64]*4], TupleShape((32, 96, 7), (32, 72, 7), (32, 96, 4), (32, 72, 4))>, <Int64, NoShape>) -> (<Tensor[Float64], (32, 96, 7)>)
      #scope: (Default)
  %1(CNode_20) = TupleGetItem(%para0_Parameter_9, I64(1))
      : (<Tuple[Tensor[Float64]*4], TupleShape((32, 96, 7), (32, 72, 7), (32, 96, 4), (32, 72, 4))>, <Int64, NoShape>) -> (<Tensor[Float64], (32, 72, 7)>)
      #scope: (Default)
  %2(CNode_21) = TupleGetItem(%para0_Parameter_9, I64(2))
      : (<Tuple[Tensor[Float64]*4], TupleShape((32, 96, 7), (32, 72, 7), (32, 96, 4), (32, 72, 4))>, <Int64, NoShape>) -> (<Tensor[Float64], (32, 96, 4)>)
      #scope: (Default)
  %3(CNode_22) = TupleGetItem(%para0_Parameter_9, I64(3))
      : (<Tuple[Tensor[Float64]*4], TupleShape((32, 96, 7), (32, 72, 7), (32, 96, 4), (32, 72, 4))>, <Int64, NoShape>) -> (<Tensor[Float64], (32, 72, 4)>)
      #scope: (Default)

#------------------------> 1
  %4(CNode_23) = Parameter_8(%0, %1, %2, %3)
      : (<Tensor[Float64], (32, 96, 7)>, <Tensor[Float64], (32, 72, 7)>, <Tensor[Float64], (32, 96, 4)>, <Tensor[Float64], (32, 72, 4)>) -> (<null>)
      #scope: (Default)
  Return(%4)
      : (<null>)
      #scope: (Default)
}
# Order:
#   1: @UnpackCall_7:CNode_23{[0]: param_Parameter_8, [1]: CNode_19, [2]: CNode_20, [3]: CNode_21, [4]: CNode_22}
#   2: @UnpackCall_7:CNode_24{[0]: ValueNode<Primitive> Return, [1]: CNode_23}


subgraph attr:
subgraph instance: after_grad_10 : 0000018372323AF0
# In file E:\Codearea\AIbase\Informer2020\.venv\Lib\site-packages\mindspore\ops\composite\base.py:595~597, 20~56/                    @jit/
subgraph @after_grad_10(%para0_args0, %para0_args1, %para0_args2, %para0_args3) {
  %0(CNode_25) = resolve(NameSpace[SymbolStr: 'Namespace:mindspore.ops.composite.base..<after_grad>'], grad_)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file E:\Codearea\AIbase\Informer2020\.venv\Lib\site-packages\mindspore\ops\composite\base.py:597, 31~36/                        return grad_(fn, weights)(*args)/
  %1(CNode_26) = resolve(NameSpace[SymbolStr: 'Namespace:mindspore.ops.composite.base..<after_grad>'], fn)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file E:\Codearea\AIbase\Informer2020\.venv\Lib\site-packages\mindspore\ops\composite\base.py:597, 37~39/                        return grad_(fn, weights)(*args)/
  %2(CNode_27) = resolve(NameSpace[SymbolStr: 'Namespace:mindspore.ops.composite.base..<after_grad>'], weights)
      : (<External, NoShape>, <External, NoShape>) -> (<Tuple[Ref[Tensor[Float32]]*65], TupleShape((512, 7, 1, 3), (512, 4), (512, 7, 1, 3), (512, 4), (512, 512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (2048, 512, 1, 1), (512, 2048, 1, 1), (512), (512), (512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (2048, 512, 1, 1), (512, 2048, 1, 1), (512), (512), (512), (512), (512, 512, 1, 3), (512), (512), (512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (2048, 512, 1, 1), (512, 2048, 1, 1), (512), (512), (512), (512), (512), (512), (512), (512), (7, 512), (7))>)
      #scope: (Default)
      # In file E:\Codearea\AIbase\Informer2020\.venv\Lib\site-packages\mindspore\ops\composite\base.py:597, 41~48/                        return grad_(fn, weights)(*args)/
  %3(CNode_28) = %0(%1, %2)
      : (<Func, NoShape>, <Tuple[Ref[Tensor[Float32]]*65], TupleShape((512, 7, 1, 3), (512, 4), (512, 7, 1, 3), (512, 4), (512, 512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (2048, 512, 1, 1), (512, 2048, 1, 1), (512), (512), (512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (2048, 512, 1, 1), (512, 2048, 1, 1), (512), (512), (512), (512), (512, 512, 1, 3), (512), (512), (512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (2048, 512, 1, 1), (512, 2048, 1, 1), (512), (512), (512), (512), (512), (512), (512), (512), (7, 512), (7))>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file E:\Codearea\AIbase\Informer2020\.venv\Lib\site-packages\mindspore\ops\composite\base.py:597, 31~49/                        return grad_(fn, weights)(*args)/
  %4(CNode_29) = MakeTuple(%para0_args0, %para0_args1, %para0_args2, %para0_args3)
      : (<Tensor[Float64], (32, 96, 7)>, <Tensor[Float64], (32, 72, 7)>, <Tensor[Float64], (32, 96, 4)>, <Tensor[Float64], (32, 72, 4)>) -> (<Tuple[Tensor[Float64]*4], TupleShape((32, 96, 7), (32, 72, 7), (32, 96, 4), (32, 72, 4))>)
      #scope: (Default)
      # In file E:\Codearea\AIbase\Informer2020\.venv\Lib\site-packages\mindspore\ops\composite\base.py:596, 36~40/                    def after_grad(*args):/

#------------------------> 2
  %5(CNode_30) = DoUnpackCall(%3, %4)
      : (<Func, NoShape>, <Tuple[Tensor[Float64]*4], TupleShape((32, 96, 7), (32, 72, 7), (32, 96, 4), (32, 72, 4))>) -> (<null>)
      #scope: (Default)
      # In file E:\Codearea\AIbase\Informer2020\.venv\Lib\site-packages\mindspore\ops\composite\base.py:597, 31~56/                        return grad_(fn, weights)(*args)/
  Return(%5)
      : (<null>)
      #scope: (Default)
      # In file E:\Codearea\AIbase\Informer2020\.venv\Lib\site-packages\mindspore\ops\composite\base.py:597, 24~56/                        return grad_(fn, weights)(*args)/
}
# Order:
#   1: @after_grad_10:CNode_25{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> SymbolStr: 'Namespace:mindspore.ops.composite.base..<after_grad>', [2]: ValueNode<Symbol> grad_}
#   2: @after_grad_10:CNode_26{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> SymbolStr: 'Namespace:mindspore.ops.composite.base..<after_grad>', [2]: ValueNode<Symbol> fn}
#   3: @after_grad_10:CNode_27{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> SymbolStr: 'Namespace:mindspore.ops.composite.base..<after_grad>', [2]: ValueNode<Symbol> weights}
#   4: @after_grad_6:CNode_31{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#   5: @after_grad_10:CNode_28{[0]: CNode_25, [1]: CNode_26, [2]: CNode_27}
#   6: @after_grad_10:CNode_32{[0]: ValueNode<UnpackGraphPrimitive> UnpackGraph, [1]: CNode_26, [2]: CNode_29}
#   7: @after_grad_10:CNode_30{[0]: ValueNode<Primitive> DoUnpackCall, [1]: CNode_28, [2]: CNode_29}
#   8: @after_grad_10:CNode_33{[0]: ValueNode<Primitive> Return, [1]: CNode_30}


subgraph attr:
core: 1
subgraph instance: UnpackCall_11 : 0000018372328E60

subgraph @UnpackCall_11(%para0_Parameter_12, %para0_Parameter_13) {
  %0(CNode_34) = TupleGetItem(%para0_Parameter_13, I64(0))
      : (<Tuple[Tensor[Float64]*4], TupleShape((32, 96, 7), (32, 72, 7), (32, 96, 4), (32, 72, 4))>, <Int64, NoShape>) -> (<Tensor[Float64], (32, 96, 7)>)
      #scope: (Default)
  %1(CNode_35) = TupleGetItem(%para0_Parameter_13, I64(1))
      : (<Tuple[Tensor[Float64]*4], TupleShape((32, 96, 7), (32, 72, 7), (32, 96, 4), (32, 72, 4))>, <Int64, NoShape>) -> (<Tensor[Float64], (32, 72, 7)>)
      #scope: (Default)
  %2(CNode_36) = TupleGetItem(%para0_Parameter_13, I64(2))
      : (<Tuple[Tensor[Float64]*4], TupleShape((32, 96, 7), (32, 72, 7), (32, 96, 4), (32, 72, 4))>, <Int64, NoShape>) -> (<Tensor[Float64], (32, 96, 4)>)
      #scope: (Default)
  %3(CNode_37) = TupleGetItem(%para0_Parameter_13, I64(3))
      : (<Tuple[Tensor[Float64]*4], TupleShape((32, 96, 7), (32, 72, 7), (32, 96, 4), (32, 72, 4))>, <Int64, NoShape>) -> (<Tensor[Float64], (32, 72, 4)>)
      #scope: (Default)

#------------------------> 3
  %4(CNode_38) = Parameter_12(%0, %1, %2, %3)
      : (<Tensor[Float64], (32, 96, 7)>, <Tensor[Float64], (32, 72, 7)>, <Tensor[Float64], (32, 96, 4)>, <Tensor[Float64], (32, 72, 4)>) -> (<null>)
      #scope: (Default)
  Return(%4)
      : (<null>)
      #scope: (Default)
}
# Order:
#   1: @UnpackCall_11:CNode_38{[0]: param_Parameter_12, [1]: CNode_34, [2]: CNode_35, [3]: CNode_36, [4]: CNode_37}
#   2: @UnpackCall_11:CNode_39{[0]: ValueNode<Primitive> Return, [1]: CNode_38}


subgraph attr:
core: 1
k_graph: 1
args_no_expand: 1
subgraph instance: grad_forward_fn_14 : 0000018372323560
# In file E:\Codearea\AIbase\Informer2020\exp\exp_informer_mindspore.py:146~150, 8~23/        def forward_fn(batch_x, batch_y, batch_x_mark, batch_y_mark):/
subgraph @grad_forward_fn_14 parent: [subgraph @grad_forward_fn_40](%para0_grad_forward_fn, %para0_grad_forward_fn, %para0_grad_forward_fn, %para0_grad_forward_fn) {
  %0(CNode_41) = J(%para_Parameter_42) primitive_attrs: {side_effect_propagate: I64(1)}
      : (<Func, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)

#------------------------> 4
  %1(CNode_43) = %0(%para0_grad_forward_fn, %para0_grad_forward_fn, %para0_grad_forward_fn, %para0_grad_forward_fn)
      : (<Tensor[Float64], (32, 96, 7)>, <Tensor[Float64], (32, 72, 7)>, <Tensor[Float64], (32, 96, 4)>, <Tensor[Float64], (32, 72, 4)>) -> (<null>)
      #scope: (Default)
  %2(CNode_44) = TupleGetItem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
  %3(CNode_45) = TupleGetItem(%1, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
  %4(CNode_46) = HyperMapPy_hyper_map[_ones_like_for_grad]{fn_leaf: MultitypeFuncGraph__ones_like_for_grad{(TypeType), (CSRTensor), (Number), (COOTensor), (Tensor), (Func), (NoneType)}}(%2)
      : (<null>) -> (<null>)
      #scope: (Default)
  %5(CNode_47) = %3(%4)
      : (<null>) -> (<null>)
      #scope: (Default)
  %6(CNode_48) = TupleGetItem(%5, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
  %7(CNode_49) = Partial(MultitypeFuncGraph_env_get{(EnvType, Tensor), (EnvType, MapTensor)}, %6) primitive_attrs: {side_effect_propagate: I64(1)}
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
  %8(CNode_50) = HyperMap_hyper_map(%7, $(@grad_forward_fn_40:para_Parameter_51))
      : (<null>, <Tuple[Ref[Tensor[Float32]]*65], TupleShape((512, 7, 1, 3), (512, 4), (512, 7, 1, 3), (512, 4), (512, 512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (2048, 512, 1, 1), (512, 2048, 1, 1), (512), (512), (512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (2048, 512, 1, 1), (512, 2048, 1, 1), (512), (512), (512), (512), (512, 512, 1, 3), (512), (512), (512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (512, 512), (512), (2048, 512, 1, 1), (512, 2048, 1, 1), (512), (512), (512), (512), (512), (512), (512), (512), (7, 512), (7))>) -> (<null>)
      #scope: (Default)
  %9(CNode_52) = MakeTuple(%2, %8)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
  Return(%9)
      : (<null>)
      #scope: (Default)
}
# Order:
#   1: @grad_forward_fn_14:CNode_43{[0]: CNode_41, [1]: param_grad_forward_fn, [2]: param_grad_forward_fn, [3]: param_grad_forward_fn, [4]: param_grad_forward_fn}
#   2: @grad_forward_fn_14:CNode_44{[0]: ValueNode<Primitive> TupleGetItem, [1]: CNode_43, [2]: ValueNode<Int64Imm> 0}
#   3: @grad_forward_fn_14:CNode_45{[0]: ValueNode<Primitive> TupleGetItem, [1]: CNode_43, [2]: ValueNode<Int64Imm> 1}
#   4: @grad_forward_fn_14:CNode_46{[0]: ValueNode<HyperMapPy> MetaFuncGraph-hyper_map[_ones_like_for_grad].53, [1]: CNode_44}
#   5: @grad_forward_fn_14:CNode_47{[0]: CNode_45, [1]: CNode_46}
#   6: @grad_forward_fn_14:CNode_48{[0]: ValueNode<Primitive> TupleGetItem, [1]: CNode_47, [2]: ValueNode<Int64Imm> 0}
#   7: @grad_forward_fn_14:CNode_49{[0]: ValueNode<Primitive> Partial, [1]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-env_get.54, [2]: CNode_48}
#   8: @grad_forward_fn_14:CNode_50{[0]: ValueNode<HyperMap> MetaFuncGraph-hyper_map.55, [1]: CNode_49, [2]: param_Parameter_51}
#   9: @grad_forward_fn_14:CNode_52{[0]: ValueNode<Primitive> MakeTuple, [1]: CNode_44, [2]: CNode_50}
#  10: @grad_forward_fn_14:CNode_56{[0]: ValueNode<Primitive> Return, [1]: CNode_52}


subgraph attr:
defer_inline: 1
subgraph instance: forward_fn_5 : 0000018372325C50
# In file E:\Codearea\AIbase\Informer2020\exp\exp_informer_mindspore.py:146~150, 8~23/        def forward_fn(batch_x, batch_y, batch_x_mark, batch_y_mark):/
subgraph @forward_fn_5(%para0_batch_x, %para0_batch_y, %para0_batch_x_mark, %para0_batch_y_mark) {
  %0(CNode_57) = resolve(NameSpace[SymbolStr: 'Namespace:exp.exp_informer_mindspore..<forward_fn>'], criterion)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file E:\Codearea\AIbase\Informer2020\exp\exp_informer_mindspore.py:149, 19~28/            loss = criterion(pred, true)/
  %1(CNode_58) = resolve(NameSpace[CommonOPS: 'Namespace:mindspore._extends.parse.trope'], getitem)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file E:\Codearea\AIbase\Informer2020\exp\exp_informer_mindspore.py:147~148, 12~73/            pred, true = self._process_one_batch(/
  %2(CNode_59) = resolve(NameSpace[SymbolStr: 'Namespace:exp.exp_informer_mindspore..<forward_fn>'], self)
      : (<External, NoShape>, <External, NoShape>) -> (<External, NoShape>)
      #scope: (Default)
      # In file E:\Codearea\AIbase\Informer2020\exp\exp_informer_mindspore.py:147, 25~29/            pred, true = self._process_one_batch(/

#------------------------> 5
  %3(CNode_60) = getattr(%2, "_process_one_batch")
      : (<External, NoShape>, <String, NoShape>) -> (<null>)
      #scope: (Default)
      # In file E:\Codearea\AIbase\Informer2020\exp\exp_informer_mindspore.py:147, 25~48/            pred, true = self._process_one_batch(/
  %4(CNode_61) = resolve(NameSpace[SymbolStr: 'Namespace:exp.exp_informer_mindspore..<forward_fn>'], train_data)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file E:\Codearea\AIbase\Informer2020\exp\exp_informer_mindspore.py:148, 16~26/                train_data, batch_x, batch_y, batch_x_mark, batch_y_mark)/
  %5(CNode_62) = %3(%4, %para0_batch_x, %para0_batch_y, %para0_batch_x_mark, %para0_batch_y_mark)
      : (<null>, <Tensor[Float64], (32, 96, 7)>, <Tensor[Float64], (32, 72, 7)>, <Tensor[Float64], (32, 96, 4)>, <Tensor[Float64], (32, 72, 4)>) -> (<null>)
      #scope: (Default)
      # In file E:\Codearea\AIbase\Informer2020\exp\exp_informer_mindspore.py:147~148, 25~73/            pred, true = self._process_one_batch(/
  %6(pred) = %1(%5, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file E:\Codearea\AIbase\Informer2020\exp\exp_informer_mindspore.py:147~148, 12~73/            pred, true = self._process_one_batch(/
  %7(true) = %1(%5, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file E:\Codearea\AIbase\Informer2020\exp\exp_informer_mindspore.py:147~148, 12~73/            pred, true = self._process_one_batch(/
  %8(loss) = %0(%6, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file E:\Codearea\AIbase\Informer2020\exp\exp_informer_mindspore.py:149, 19~40/            loss = criterion(pred, true)/
  Return(%8)
      : (<null>)
      #scope: (Default)
      # In file E:\Codearea\AIbase\Informer2020\exp\exp_informer_mindspore.py:150, 12~23/            return loss/
}
# Order:
#   1: @forward_fn_5:CNode_60{[0]: ValueNode<Primitive> getattr, [1]: CNode_59, [2]: ValueNode<StringImm> _process_one_batch}
#   2: @forward_fn_5:CNode_61{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> SymbolStr: 'Namespace:exp.exp_informer_mindspore..<forward_fn>', [2]: ValueNode<Symbol> train_data}
#   3: @forward_fn_5:CNode_63{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#   5: @forward_fn_5:CNode_62{[0]: CNode_60, [1]: CNode_61, [2]: param_batch_x, [3]: param_batch_y, [4]: param_batch_x_mark, [5]: param_batch_y_mark}
#   6: @forward_fn_5:CNode_58{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> getitem}
#   7: @forward_fn_5:pred{[0]: CNode_58, [1]: CNode_62, [2]: ValueNode<Int64Imm> 0}
#   8: @forward_fn_5:true{[0]: CNode_58, [1]: CNode_62, [2]: ValueNode<Int64Imm> 1}
#   9: @forward_fn_5:CNode_57{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> SymbolStr: 'Namespace:exp.exp_informer_mindspore..<forward_fn>', [2]: ValueNode<Symbol> criterion}
#  10: @forward_fn_5:CNode_64{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#  12: @forward_fn_5:loss{[0]: CNode_57, [1]: pred, [2]: true}
#  13: @forward_fn_5:CNode_65{[0]: ValueNode<Primitive> Return, [1]: loss}
#  14: @forward_fn_5:CNode_66{[0]: ValueNode<Primitive> MakeTuple, [1]: ValueNode<StringImm> __py_exec_index1_getattr__, [2]: ValueNode<StringImm> __py_exec_index2_getattr__}
#  15: @forward_fn_5:CNode_67{[0]: ValueNode<Primitive> MakeTuple, [1]: CNode_59, [2]: ValueNode<StringImm> _process_one_batch}
#  16: @forward_fn_5:CNode_68{[0]: ValueNode<Primitive> make_dict, [1]: CNode_66, [2]: CNode_67}


# ===============================================================================================
# The total of function graphs in evaluation stack: 6/9 (Ignored 3 internal frames).
# ===============================================================================================


# ===============================================================================================
# The rest function graphs are the following:
# ===============================================================================================
No more function graphs.

